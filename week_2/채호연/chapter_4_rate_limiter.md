# Chapter 4 : 처리율 제한 장치의 설계

## I. 처리율 제한 장치(Rate Limiter)란?
1. 정의
 - 클라이언트 또는 서비스가 보내는 트래픽의 처리율을 제어하기 위한 장치
 - 사용 목적 및 처리율 제한 대상에 따라 다양한 작동 방식 및 조건이 있지만, '제한 장치에 정의된 임계치(Threshold)를 초과하는 호출/요청은 모두 Block' 이라는 골자는 모두 공통적으로 가짐.

2. 사용 시 장점
 - DoS(Denial of Service) 공격에 의한 서버 자원 고갈 방지 가능
   - ex. 구글 독스의 경우 분당 300회의 Read 요청, 트위터는 3시간 이내 300개의 트윗 업로드(Write) 만 가능하도록 제한하고 있다.
 - 효율적 자원 사용을 가능케 하여 비용 절감 가능
> [!NOTE]
> _**DoS(Denial of Service)**_
> 
> 대상 기계/자원에 대해 막대한 양의 호출/요청을 보내, 장애를 유발하고자 하는 악의적 공격 행위

## II. 요구사항 정의
> 현재 학습 대상인 _가상 면접 사례로 배우는 대규모 시스템 설계 기초_ 의 해당 챕터 내에서 등장하는 가상 면접관과 피면접자의 대화 부분은 생략하고 요점만 정리하겠습니다.

**요구사항**
 - 설정된 임계치를 초과하는 요청은 정확하게 제한.
 - 낮은 응답시간 : 처리율 제한 장치의 활동이 서비스 비즈니스 로직의 HTTP 응답 시간을 저하시켜선 안된다.
 - 상기한 항목과 같은 맥락으로, 가능한 적은 메모리를 써야한다.
 - 분산 시스템에서의 사용을 전제한다. : 하나의 처리율 제한 장치를 여러 서버나 프로세스에서 공유할 수 있어야 한다.
 - 예외 처리 : 요청이 제한되었을 때는 그 사실을 사용자에게 분명하게 알려야 한다.
 - 높은 Fault-tolerancy : 제한 장치에 장애가 발생하더라도 복구 가능해야 하며, 전체 시스템에 영향을 주어서는 안된다.

## III. 설계 시 고려할 점
### 처리율 제한 장치의 위치

크게 세 가지 위치를 고려해볼 수 있다.

1. **Client-side**
<img width="988" height="411" alt="Screenshot 2025-11-22 at 10 57 24 PM" src="https://github.com/user-attachments/assets/b667432f-9414-4550-9809-75491abb0c2d" />

 - 책에서는 이 위치에 처리율 제한 장치를 두는 것이 바람직하지 않다고 한다. 이를 이해해보기 위해 다음의 상황을 가정해보았고, 그 결과 '우회 가능하다'는 단점으로 인해 부적절함을 이해하게 되었다.
   - Client-side 의 Javascript 코드로 '특정 버튼 1초 이내 10번 클릭'을 제한했다고 해보자.
   - 하지만 이는 1) 웹 브라우저 도구를 통해 Javascript 코드를 우회하거나, 2) API Endpoint 로 브라우저를 통하지 않고 직접 요청을 보내는 방식으로 우회 가능하다.
   
2. **Server-side**
<img width="867" height="285" alt="Screenshot 2025-11-22 at 10 58 20 PM" src="https://github.com/user-attachments/assets/5eb220bc-661b-48c9-928f-8445c775f140" />

 - 서버 측에 설치할 수도 있다. 코드 수준에서도 가능하고, 별도의 컴포넌트를 활용할 수도 있다.
 - 이 경우엔 적절한 위치에, 적절한 시점에 HTTP 요청이 Rate Limiter 를 통과하도록 설계한다면, 우회 가능성을 최소화할 수 있다는 점에서 Client-side 보다 우위를 가진다.
 - 하지만, Server 와 같은 자원을 공유하는 Machine 상에서 Rate Limiter 를 작동시키는 경우, Rate Limiter 가 받아내야 하는 전체 요청으로 인한 부하가 결국 서버로의 부하로 이어질 수 있으므로 주의해야 한다.

3. **Middleware**
<img width="1011" height="272" alt="Screenshot 2025-11-22 at 11 03 52 PM" src="https://github.com/user-attachments/assets/63e32dae-2e46-4b22-a30e-eee4aa924fb9" />

- Middleware 형태의 Rate limiter 를 활용할 경우, Client-side 와 Server-side 의 단점을 극복할 수 있다.
- Cloud 환경에서의 마이크로서비스 아키텍처로 설계된 시스템의 경우, API Gateway 에게 처리율 제한 책임을 부여하기도 한다.

처리율 제한 장치의 위치를 정할 땐 다음의 사항을 고려하는 것이 바람직하다.
 - 프로그래밍 언어, 캐시 서비스 등 현행 서비스가 활용 중인 기술 스택
 - 무엇보다도, 비즈니스 요구와 가장 잘 부합하는 것이 중요.

## IV. 대표적인 Rater Limiting Algorithms

### 토큰 버킷(Token Bucket)
1. **알고리즘의 핵심 발상** ~~학교 앞 술집 화장실 열쇠~~
 - 용량이 유한한 컨테이너에 담긴 토큰을 주기적으로 채운다.
 - 각 요청 당 토큰 하나를 가져간다고 생각한다.
 - 토큰 컨테이너가 비었다는 것은 '처리율 제한기가 정한 기간 내 요청 임계에 도달했다는 것' -> 비었을 때 도달한 요청은 Block.

<img width="583" height="487" alt="Screenshot 2025-11-22 at 11 31 37 PM" src="https://github.com/user-attachments/assets/ffe10e60-8928-4061-9c83-9504638f5277" />

2. **토큰 버킷 알고리즘을 구성하는 두 가지 요소** - _버킷의 크기_ & _토큰 공급률(refill rate)_

이 두 가지 요소가 함께 '처리율 제한 장치의 임계'를 정한다.
 - 토큰 공급률 : 처리율 제한 장치의 시간 임계 정책이다.
 - 버킷의 크기 : 처리율 제한 장치의 처리량 임계 정책이다.

가령, 앞서 인용했던 트위터의 '3시간 이내 300 개 이내의 트윗만 게시 가능하다' 라는 부분을 위의 두 가지 요소로 구분하여 나타내면 다음과 같다.
 - 토큰 공급률 : 3시간 당 300개의 토큰을 버킷에 공급한다. 각 토큰은 트윗을 게시하는 API Endpoint 로의 POST 요청이 진입할 때 하나씩 차감될 것이다.
 - 버킷의 크기 : 총 300개의 토큰만 버킷에 저장된다.

이 두 가지를 위와 같은 방식으로 활용한다면, 컨테이너의 자료구조 또는 토큰의 구조/형태와 무관하게 '토큰 버킷 알고리즘'을 활용한 처리율 제한기라고 생각할 수 있다.

실제로, 처리율 제한 장치의 제한 정책에 따라 다양하게 구현할 수 있다.
예를 들어, 증권 거래소의 실시간 시세 조회 API 대상 Rate Limiter 를 토큰 버킷 알고리즘을 활용하여 구현한다면 개략적으로 다음과 같은 도식으로 표현할 수 있을 것이다.

<img width="354" height="220" alt="Screenshot 2025-11-22 at 11 43 16 PM" src="https://github.com/user-attachments/assets/57cbc01c-73be-4686-b9aa-cafc90dcd662" />

3. **토큰 버킷 : 장점**
 - 구현이 쉽다.
 - 메모리 사용 측면에서도 효율적
 - 짧은 시간에 집중되는 트래픽 패턴에도 대응 가능

4. **토큰 버킷 : 단점**
 - 버킷 크기와 토큰 공급률이라는 두 가지 독립변수에 따라 '자원 활용 효율성'과 '서비스 만족도(너무 타이트하면 사용자들이 실패를 빈번하게 겪을 것이므로)' 라는 두 가지 종속 변수가 변화함.
   - 이를 적절히 조정해가며 최적화하는 것이 까다로움.

### 누출 버킷(Leaky Bucket)
1. 알고리즘 핵심 발상 : '요청 처리율(Outflow Rate)'에 초점을 둔 알고리즘. 우리가 아는 대기열
 - 일반적으로 FIFO Queue 를 활용한다.
 - 요청이 도착하면, Q에 빈 자리가 있는지 확인한다. 없는 경우, 요청은 Drop 된다.
 - 지정된 시간마다, 큐에서 지정된 양의 작업을 꺼내어 처리한다.
   
<img width="608" height="207" alt="Screenshot 2025-11-22 at 11 54 55 PM" src="https://github.com/user-attachments/assets/91152847-81c6-4632-b333-b3a944a0107a" />

2. **누출 버킷을 구성하는 두 가지 요소** - _Queue의 크기_ & _요청 처리율(Outflow Rate)_

토큰 버킷 알고리즘과 마찬가지로, 위의 두 가지 요소가 누출 버킷의 처리율 제한 정책을 구성한다.

 - Queue의 크기 : 처리 대기 중인 요청의 수와 비례한다. 그렇기 때문에 '요청 별 대기 시간'과도 직결된다.
 - 요청 처리율 : 큐로부터 어떤 시간 주기에 따라 얼만큼의 작업을 꺼내 처리할지를 표현한다. 마찬가지로  '요청 별 대기 시간'과도 직결된다.

3. **장점**
 - 큐의 크기가 제한되어 있어 메모리 사용량 측면에서 효율적 & 관리 용이
 - 고정된 처리율을 갖고 있기 때문에 '안정적 출력(Stable Outflow Rate)가 필요한 경우에 적합한다.

4. **단점**
 - 단시간에 많은 트래픽이 몰리는 경우 -> 큐의 크기가 커지거나 큐에 많은 요청이 누적 -> 최신 요청들은 버려지게 됨 -> 사용자 만족도 하락
 - 두 요소를 적절히 조정하여 '사용자 대기 시간 최적화' 와 '서비스 안정성' 사이에 균형 지점을 찾는 것이 상당히 어려움.

### 고정 윈도우 카운터(Fixed Window Counter)
1. 알고리즘 핵심 발상 : 처리율 제한 기간을 '윈도우'로 생각하고, 해당 기간 내 요청이 들어올 때마다 윈도우 별 카운터를 1씩 증가.
 - 시간 축을 '고정된 간격의 윈도우'로 나눈다. : 처리율 제한 정책의 '기간 정책'에 해당.
 - 각 윈도우 별로 '해당 시간 구간 내에 들어온 요청의 수'를 표현하는 카운터를 붙인다.
 - 요청이 접수될 때마다 카운터를 증가시킨다.
 - 윈도우 별로 설정된 카운터 임계에 도달할 경우, 새로운 요청은 버려진다.

<img width="620" height="346" alt="Screenshot 2025-11-23 at 12 16 30 AM" src="https://github.com/user-attachments/assets/bbe862f3-0ba1-4f0c-ac5c-c09e2271c2dd" />

2. **고정 윈도우 카운터 알고리즘을 구성하는 두 가지 요소** - _윈도우(Window)_ & _카운터_
 - 윈도우 : 처리율 제한 정책 상에서 정의한 처리율 임계가 성립하는 시간 길이를 의미한다. 만약 '1초 당 10개의 요청 제한' 정책이라면, 윈도우의 크기는 1초가 되어야 한다.
 - 카운터 : 윈도우가 표현하는 시간 내에 유입된 요청의 수를 의미한다.

3. **장점**
 - 메모리 및 연산 효율이 좋다.
 - 이해하기 쉽다.

4. **단점**
 - 윈도 경계 부근에서 일시적으로 많은 트래픽이 몰려드는 경우, 기대했던 시스템의 처리 한도보다 많은 양의 요청이 처리되는 Burst 발생 가능

이를 도식으로 살펴보면 다음과 같다.

<img width="615" height="271" alt="Screenshot 2025-11-23 at 12 24 56 AM" src="https://github.com/user-attachments/assets/f61e135e-f051-420b-956d-39767e4e0513" />

해당 도식은 '1분 당 5회 초과 요청 제한' 이라는 정책을 따르는 고정 윈도우 카운터 알고리즘 방식의 처리율 제한 장치의 윈도우 별 카운터 기록을 보여준다.

위의 도식 상에서 02:00:30~02:00:59 (02:00:00 에 시작하는 윈도우가 끝나기 전에 5회의 요청 발생) 윈도우와 02:01:00~02:01:30 구간을 살펴보면 된다.

각각의 구간을 A, B 로 지칭하겠다.

도식을 보면, A 윈도우에서 5회의 요청이 처리되고(임계 이내), 02:01:00이 되는 순간 카운터가 초기화된다. 그렇기 때문에 B 윈도우에서 5회의 추가 요청을 더 수용 가능하다.

하지만 실제로 A~B 구간을 합치면 정책 상 정의하는 윈도우의 길이인 1분이 된다. 그렇기 때문에, 실제로는 이 1분 동안 처리율 제한 장치가 정한 임계인 5개 보다 두 배 많은 10 개의 요청이 처리된 것이다.

즉, 정책에서 정의한 처리율 제한을 일시적으로 초과하게 된다.


### 슬라이딩 윈도우 로그(Sliding Window Log)

1. **알고리즘 핵심 발상** : 요청 기록을 시간 순으로 크기가 정해진 '로그'에 기록한다. 로그의 크기가 곧 처리율 제한 임계가 된다. 로그 내에서 정책에서 설정한 시간이 지난 것은 만료 시킨다.
 - 요청이 들어온다.
 - 들어온 시점의 타임스탬프를 계산한다.
 - 새 요청이 들어온 시점에 '로그' 내에 기록된 요청 타임스탬프 중, '윈도우' 기간을 벗어난 오래된 요청 타임스탬프를 만료시킨다.
  - 이때, 로그 내에 유효한 타임스탬프의 수가 '임계 미만'인 경우, 새 요청의 타임스탬프를 로그에 기록하고 요청을 통과시킨다.
  - 만약 임계에 도달했다면, 타임 스탬프를 로그에 기록하고, 요청은 거절된다.

<img width="524" height="424" alt="Screenshot 2025-11-23 at 12 32 54 AM" src="https://github.com/user-attachments/assets/36fd0fa9-c7ae-4776-9439-d6eaf325c17f" />

2. **알고리즘 구성 요소** : _로그 크기_ & _윈도우_
 - 로그 크기 : 윈도우 내에 허용 가능한 요청의 타임 스탬프를 기록하는 공간. 이 로그 크기를 토대로 임계 초과 여부 판별이 이루어진다.
   - Timestamp 의 선입선출/만료 처리라는 특성으로 인해, Redis 의 Sorted Set 이 주로 사용된다.
 - 윈도우 : 처리율 제한 정책 중 '기간 정책'을 의미.

3. **장점**
 - 고정 윈도우 카운터 알고리즘의 단점이었던 '윈도우 경계 부분 Burst 발생' 가능성을 차단할 수 있다. -> 임계 도달 시 요청은 거부하지만, 타임스탬프를 로그에 남기는 이유가 바로 이것.

4. **단점**
 - 거부된 요청의 타임스탬프로 인해 상기한 알고리즘보다 메모리 집약적이다.

### 슬라이딩 윈도우 카운터(Sliding Window Counter)

1. **알고리즘 핵심 발상** : 고정 윈도우 카운터 + 이동 윈도우 로깅
> 크게 두 가지 구현 방식이 있는데, 그 중 하나만 소개.
 - 윈도우 내의 요청 수를 기록하는 방식은 고정 윈도우 카운터와 동일하다. 윈도우 별 카운터가 존재하고 이를 갱신해가는 방식이다.
 - 다만, 임계 도달 여부 판단 시에 다음과 같은 절차를 거친다.
   - 현 시점 B 윈도우에 속해있고, 현재 B 윈도우 전체 길이 중 30% 지점이라고 가정하자. 또한, 윈도우의 길이는 1분으로 가정해보자.
   - 그렇다면, '지금 시점 기준 끝나는 윈도우 내라면 현재 요청 수가 허용이 되는가?' 를 계산해보는 것이다. 즉,
     - 직전 윈도우인 A 윈도우 요청 카운터의 일부분(70%) + 현재 윈도우 내에서 현 지점의 기간 비중(30%)를 더해 새로운 100% 내에서 요청이 얼마나 들어왔는지 보는 것이다.
     - 현재 지나고 있는 윈도우 시작 지점으로부터 현 시점까지의 길이가 윈도우 구간 내에서 차지하는 비중을 k%(여기선 30)이라고 한다면 다음과 같은 수식을 통해 계산할 수 있다.
     - _(현 윈도우의 k% 까지의 요청 카운터) + (이전 윈도우 A 전체 카운터 * (100 - k))_
 
<img width="615" height="350" alt="Screenshot 2025-11-23 at 12 29 46 AM" src="https://github.com/user-attachments/assets/74fff377-ac64-46e0-9b0b-da6492be404e" />

2. 장점
 - 이전 시간대의 평균 처리율에 따라 현재 윈도의 상태를 계산 -> 짧은 시간 몰리는 트래픽 버스트 해결 가능
 - 메모리 효율이 좋음

3. 단점
 - 수식 자체가 '요청의 균등한 분포'를 전제함. -> 임계 초과 판별 기준이 완벽하게 정확하진 않음.
  - 하지만, Cloudflare 의 실험에 따르면 40억 건의 요청 중, 임계 판별 기준을 완벽하게 준수하지 않고도 통과하거나/버려진 요청의 비율이 0.003%에 불과 -> 허용 가능한 수치.

## V. 1차 설계

처리율 제한 장치를 개략적으로 설계하기 위해선 크게 다음의 세 질문에 대답해야 한다.

하나는 앞서 언급했던 '처리율 제한 장치를 어디에 둘 것인가?' 이다.

두 번째와 세 번째는 각각 다음과 같다.

### 처리율 제한 대상 정의
- API Endpoint 에 대한 전체 요청 수를 제한할 것인가?
- 사용자 별 서비스 진입 요청 자체를 제한할 것인가?
- IP 주소 별 요청 수를 제한할 것인가?

### 처리율 측정 카운터 및 필요 자료구조를 어디에 저장할 것인가?
처리율 측정 카운터에 대한 연산으로 인해 시스템 전체 부하가 증가하는 상황은 피해야 한다.
그런 관점에서 이러한 카운터 및 자료구조를 DB 에 저장하는 것은 적절하지 않다.

연산 자체의 리소스 집약도와 자료구조의 적절함으로 인해 Redis 가 많이 활용된다.

### 전체적인 아키텍처 및 작동 방식

<img width="600" height="234" alt="Screenshot 2025-11-23 at 1 00 43 AM" src="https://github.com/user-attachments/assets/f061f237-9d45-4fdd-9fca-b09c5dc22795" />

1) 클라이언트가 처리율 제한 미들웨어에게 요청을 보낸다.
2) 처리율 제한 미들웨어는 레디스의 지정 버킷에서 카운터를 가져와서 한도에 도달했는지 아닌지 검사한다.
   2-1) 임계 도달 -> 요청 거절.
   2-2) 임계 미도달 -> 요청을 API 서버로 Route.

### 처리율 제한 장치의 처리 결과를 어떻게 클라이언트에게 알릴 것인가?

**HTTP 응답 헤더** 를 활용해 전달한다.
 - `X-Ratelimit-Remaining` : 윈도 내에 남은 처리 가능 요청의 수
 - `X-Ratelimit-Limit` : 매 윈도마다 클라이언트가 전송할 수 있는 요청의 수
 - `X-Ratelimit-Retry-After` : 한도 제한에 걸리지 않으려면 몇 초 뒤에 요청을 다시 보내야 하는지 알림.

너무 많은 요청이 올 경우, `429 Too Many Request` 응답을 `X-Ratelimit-Retry-After` 와 함께 보내자.

### 상세 설계

<img width="620" height="484" alt="Screenshot 2025-11-23 at 1 11 26 AM" src="https://github.com/user-attachments/assets/adbb7392-04ac-4d9e-914c-33d11b7ac70a" />

1) 처리율 제한 규칙은 디스크에 보관한다. (Lyft 가 활용하는 오픈 소스의 경우, 디스크에 코드 레벨(yaml 처럼)로 정의한다.)
 - 작업 프로세스는 수시로 규칙을 디스크에서 읽어 캐시에 저장.
2) 클라이언트가 요청을 서버에 보냄 -> 실제론 처리율 제한 미들웨어에 먼저 도달
3) 처리율 제한 미들웨어는 제한 규칙을 캐시에서 가져온다. 또한, 카운터 및 마지막 요청의 타임스탬프를 레디스 캐시에서 가져온다.
4) 처리율 제한 정책 위반 여부 검증 후 적절히 조치한다.
 - 이때 위반한 요청을 버리거나, 메세지 큐에 저장할 수 있다.

## VI. 분산 환경에서의 처리율 제한 장치 : 경쟁 조건(Race Condition) & 동기화(Synchronization)

분산 환경에서 레이트 리미터가 기대되는 작동을 하기 위해선, 두 가지 문제를 극복해야 한다.

### 경쟁 조건 - feat. TOCTOU(Time of Check, Time of Use)
분산 환경에서는 Redis 에 저장된 카운터 정보를 조회해가는 시점과 갱신 후 저장을 시도하는 시점에서 경쟁 조건이 발생할 수 있다.

<img width="613" height="308" alt="Screenshot 2025-11-23 at 1 18 35 AM" src="https://github.com/user-attachments/assets/99d6a78a-71d2-4861-993e-3872f950ce18" />

이처럼, 각 분산 시스템/프로세스가 공유 자원의 정보를 조회한 시점과 가공 후 저장 시도하는 과정에서 일관성이 깨지는 문제를 TOCTOU 라고 한다.

이를 극복하기 위해선 두 가지 방법이 있다.

1. Lock : 성능 병목을 유발할 가능성이 있다.
2. Lua Script 를 활용하여 복수의 Redis 접근 자체를 원자적으로 처리
3. Sorted Set 을 활용한 Sliding Window Log 알고리즘으로 바꾸기.

### 동기화 이슈
<img width="627" height="204" alt="Screenshot 2025-11-23 at 1 22 34 AM" src="https://github.com/user-attachments/assets/7affea51-e975-4af5-bfd6-d341e0c155ed" />

분산 환경이고, 처리율 제한 장치 또한 분산 아키텍처로 구성되었다면 도식과 같은 형태일 것이다.

이때, 각 사용자/처리율 제한 대상 별 카운터 또는 제한 대상 정보 현황이 처리율 제한 장치 또는 서버에 종속되어 저장된 형태라면, 요청을 처리하는 서버/처리율 제한 장치가 바뀌었을 때 이전 정보와 연속되지 않게 된다.

이를 해결하기 위해선, 두 가지 방법이 있다.

1. Sticky Session : Stateful 해져 확장성이 떨어짐.
2. Redis 와 같은 중앙 집중형 : Way Better.

