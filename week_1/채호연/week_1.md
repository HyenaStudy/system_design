# System Design: Chapter 1 - 규모 확장성을 갖춘 설계를 위한 선수 지식

## I. 천리 길도 한 걸음부터 - 단일 서버
<img width="637" height="358" alt="Screenshot 2025-11-16 at 8 16 47 PM" src="https://github.com/user-attachments/assets/8c277183-a09e-4790-a122-7118a5436a0e" />

위의 도식은 '모든 컴포넌트가 단 한 대의 서버에서 실행'되는 시스템의 구조를 보여줍니다.

그리고 요새 워낙 복잡하고 멋진 설계가 많기 때문에 상당히 앙상해보이기도 합니다.

그렇지만 '서버가 가지고 있는 자원을 요청하고, 이 요청에 합당한 응답을 보내준다.' 라는 관점에서는 이 시스템도 충분히 정상적으로 잘 작동합니다.

이런 시스템에서는 웹, 앱, 데이터 저장소, 캐시 등이 모두 서버 한 대에서 실행됩니다.

잠시 네트워크 수업 때를 떠올려 이러한 아키텍처에서 사용자의 요청 진입 과정, 그리고 이에 대한 응답이 반환되기까지의 과정을 개략적으로 톺아봅시다.

1) 사용자는 도메인 이름으로 접속 시도를 합니다. 편의 상 웹 브라우저의 검색 창에 입력했다고 하겠습니다.
2) 사용자의 장치(이하 '호스트'로 통칭)는 DNS(Domain Name Service)에게 이 Human-readable 한 도메인 네임을 주고, DNS는 이와 매핑된 IP 주소를 돌려줍니다.
> 물론 호스트의 로컬 환경에 Caching 되는 것이 보통입니다.
3) 웹 브라우저는 IP 주소로 HTTP 요청을 보냅니다.
4) 요청을 받은 웹 서버는 HTML 페이지 또는 JSON 포맷의 응답을 반환합니다.

잘 작동하면 뭐가 문제일까요?

이 웹 서버를 사용하는 사용자가 많아질 수록 - 정확히 말하면, 웹 서버에 적대적 행위를 통해 정상적 수준을 벗어난 부하를 가하지 않는 사용자가 많아질 수록, 그래서 초당 요청 수가 많아질 수록 -,
서버가 가진 자원은 쉽게 고갈됩니다.

그렇지만 각 컴포넌트간 이처럼 높은 결합도를 가진 구조로 인해, 해당 웹 서버를 통째로 복사해서 새로운 물리 서버에 올리는 것 이외엔 이와 같은 트래픽 증가에 대응할 수 있는 방안이 없습니다.

이를 극복하기 위해 우선 결합도가 높은 컴포넌트를 하나씩 분리해가도록 하겠습니다.

분리하면, 필요에 따라 각각을 확장해나갈 수 있기 때문에, 유연한 대응이 가능할 수 있도록 합니다.

<br></br>

우선은 DB 를 분리해봅니다.

<img width="599" height="347" alt="Screenshot 2025-11-16 at 8 32 17 PM" src="https://github.com/user-attachments/assets/566ad353-2986-44f7-bc9b-2ca16d2d54ec" />

이제 우리가 처음 웹 개발 입문 시점에 만들어보았던 게시판 시스템과 유사한 형태가 보입니다.

이와 같이 트래픽을 처리하는 웹 계층과 데이터를 저장하고 반환하는 데이터 계층을 분리하면 각각이 독립적으로 확장 가능해집니다.

그럼에도 여전히 부족합니다.

하나씩 키워보며, 사용자 규모에 따라 시스템 아키텍처가 어떻게 발전하는지 살펴보도록 하겠습니다.

## II. 본격적인 시스템 규모 확장 : 무엇을, 어떻게 확장해야할까?

### 서버 Scale-out 과 Load Balancer

시스템을 구성하는 컴포넌트는 크게 두 가지 방향으로 확장이 가능합니다

_수직적 확장(Scale-up)_과 _수평적 확장(Scale-out)_ 입니다.

##### 수직적 확장
수직적 확장 또한 특정 상황에서는 충분히 유용한 전략입니다.

그렇지만 크게 두 가지 한계가 있습니다.

1) 물리적 제약으로 인해 CPU/Memory 와 같은 컴퓨팅 자원 확장이 자유롭지 못합니다.
2) 여전히 단일 물리 서버 형태이기 때문에, 장애 발생 시 서비스도 함께 중단됩니다. 즉, 단일실패지점이 되는 것입니다.

##### 수평적 확장
말 그대로 서버의 수를 늘리는 수평적 확장을 말합니다.

물리 서버의 수를 늘리는 것도, Containerize 한 서버 시스템의 수를 늘리는 것도 모두 수평적 확장에 해당합니다.

필요에 따라 서버의 수를 유동적으로 조정해 비용을 통제하기도 용이합니다.

또한 한 대의 서버에 장애가 발생하더라도, 다른 서버로 요청을 Route 하여 처리하면 되므로 장애 대응에도 용이합니다.

복수의 서버가 존재하기 때문에 필요한 것이 바로 _로드 밸런서(Load Balancer)_ 입니다.

<img width="592" height="518" alt="Screenshot 2025-11-16 at 8 45 07 PM" src="https://github.com/user-attachments/assets/91fc0493-cc07-48ec-90b3-3e183533844f" />

트래픽을 부하로 생각했을 때, 이 '부하'를 분산해서 나누어줄 대상을 '부하분산집합' 이라고 합니다.

이 부하분산집합 내에 복수의 서버가 존재하는 것이죠.

로드 밸런서는 주기적으로 부하분산집합 내의 서버가 살아있는지 체크합니다.

그리고 한 서버의 응답이 없다면, 그 서버로 분산할 부하를 집합 내의 다른 서버로 라우트하는 형태로 동작합니다.

정리해보겠습니다.

서버 또는 서버를 구성하는 컴포넌트 확장에는 크게 두 가지 방식이 있습니다. 수직적 확장과 수평적 확장입니다.

이 중 수평적 확장 시엔 복수의 서버로 적절히 요청을 분산할 책임자가 필요하고, 이를 로드밸런서라고 합니다.

이를 통해 우리는 다음의 두 가지를 성취할 수 있습니다.

1) 필요에 따라 유연하게 서버의 수를 확장 가능
2) 한 서버가 실패할 경우에도, 해당 서버로의 요청을 정상 서버로 우회시킴으로서 장애 대응이 가능 : Failover 대책 마련

그렇다면 데이터 저장 계층은 어떨까요?

데이터 저장 계층 또한 Failover 를 갖출 수 있습니다.

가장 대표적인 방식이 바로 **데이터베이스 다중화(Database Replication)** 입니다.

<img width="506" height="523" alt="Screenshot 2025-11-16 at 9 13 53 PM" src="https://github.com/user-attachments/assets/4e7a387b-5d5e-429d-af3d-0d8d4fc6c872" />

서비스가 처한 환경과 이에 따른 필요에 부합하는 다양한 다중화 방식이 있을 수 있지만, 가장 흔히 쓰이는 방식이 바로 '쓰기와 읽기 데이터베이스를 분리'하는 방식의 다중화입니다.

데이터의 수정 및 삽입은 Write DB에 대해서만 수행하고, 읽기는 Read DB에 대해서만 하는 직관적인 방식입니다.

좋은 점은, 다중화된 구조 덕분에 한 DB에 문제가 발생해도 대응이 가능해진다는 것입니다. Failover 방안을 갖추게 되는 것이죠.

그렇지만 시스템이 복잡해질 수록 고민 지점은 늘어갑니다.

대표적인 것이 바로 **Write DB 와 Read DB 간 데이터 일관성 유지** 방안입니다.

이를 위해 보통 두 가지 방식을 활용합니다.

1) 복수의 Write DB 두기 : 장애 발생 대응 역량 관점에서는 단일 Write DB 구조보다 유리합니다. 다만, 여전히 두 Write DB 간 데이터 일관성 보장이 필요합니다.
2) 원형 다중화(Circular Replication) : 최초로 변화가 발생한 Write DB 를 읽기 DB가 읽어가고, 이를 다른 Read DB가 읽어가는 방식으로 일관성을 유지합니다.

이러한 일관성 유지는 통상 DBMS 에서 제공하는 복구 및 백업 기능을 활용하거나 별도의 Recovery Script 를 개발자가 만들어야 합니다


### 데이터베이스의 확장

그렇다면 데이터베이스 자체도 확장이 가능할까요?

역시 수직적 확장과 수평적 확장 모두 가능합니다.

여기서는 수평적 확장에 좀 더 집중하여 살펴보겠습니다.

같은 데이터를 완전히 동일하게 갖고 있는 복수의 DB 를 관리하는 것이 물론 가장 안전할 수 있습니다.

하지만 앞서 언급한 데이터 일관성 유지가 좀 어려운 문제여야 말이죠.

Sharding 이라는 방식을 활용하면 이러한 부분을 조금 더 쉽게 만들어줄 수 있습니다.

서비스 또는 시스템이 활용하는 데이터를 수직 또는 수평적으로 분할하여 각각의 DB에 저장하는 것이죠.

<img width="434" height="461" alt="Screenshot 2025-11-16 at 9 39 26 PM" src="https://github.com/user-attachments/assets/f50a7ae7-086a-414c-a88f-fed08d57a30d" />

어떤 릴레이션의 K번 Row 까지는 A DB에, K+1번 째 행 부터는 B DB에 저장할 수 있습니다. 수평적 분할입니다.

어떤 릴레이션의 K번 Column 까지는 A DB에, 나머지는 B DB에 저장할 수 있습니다. 수직적 분할입니다.

이렇게 분할된 데이터를 어떻게 찾을까요? 바로 이러한 데이터를 분할하고 저장하는 기준을 정하는 샤딩 키(Sharding Key)를 정함으로써 가능해집니다.

샤딩 키를 통한 저장 위치 지정은 꼭 Hash Function 과 닮아있습니다.

그렇기 때문에 샤딩 키를 어떻게 정의하는 지도 중요합니다.

데이터가 너무 편중되지 않고 고르게 분산되어 저장되도록 정하는 것이 좋습니다.

또한 Sharding Key 에 근거한 데이터 저장/읽기 위치 지정 계산이 복잡한 것보단 단순한 편이 시간 복잡도 상 유리할 것입니다.

샤딩이 좋긴합니다만, 역시 새로운 고민이 생깁니다.
아래는 샤딩 도입에 따라 생기는 대표적인 고민들입니다.

1. 재샤딩 문제 : 어떤 샤드의 공간이 모두 소진된 상황을 _샤드 소진(Shard Exhaustion)_ 이라고 합니다. 이때는 결과가 보다 고르도록 샤드 키 계산 함수를 개선해야 합니다. Consistent Hashing 으로 해결합니다.
2. 유명인사 문제(Celebrity Problem) : 특정 키에 연산이 집중되는 현상을 말합니다. 이러한 키를 Hot Key라고 하는데, 어떤 샤드에 이런 핫 키가 많다면 해당 샤드가 유독 빠르게 소진될 수 있습니다.
3. 조인 제약 문제 : 샤딩으로 인해 조인 시 제약이 생깁니다. 의도적 비정규화를 통해 일정 부분 해결이 가능합니다.

### 캐시 계층 도입

이제 우리는 수평적으로 확장된 서버와 DB가 있습니다.

하지만 요청이 요구하는 자원이 DB에 있고, 이를 가져와 반환해야 하는 구조가 변하지 않는 이상, 사용자 규모가 커질 수록 응답 지연속도도 함께 커집니다.

이를 해결하기 위해 캐시 계층을 도입할 수 있습니다.

DB 보다 조회/쓰기/연결 면에서 더 빠른 장소를 찾아 응답 시 반환해야 하는 데이터를 저장해두고, 빠르게 반환하는 전략인 것입니다.

이러한 캐시 저장소로 가장 많이 사용되는 것이 '메모리' 입니다.

<img width="621" height="87" alt="Screenshot 2025-11-16 at 9 46 36 PM" src="https://github.com/user-attachments/assets/b91f9195-a741-462c-8696-22e1ef50a256" />

캐시 설계 시엔 다음과 같은 점을 고려해야 합니다.

1. 캐시 대상 데이터의 R/W 빈도 분석 : Write 가 더 많다면, 캐시 데이터와 원본 데이터의 일관성 보장 비용이 더 커지는 문제가 발생할 수 있습니다. 물론, 아예 저장을 캐시에만 할 수 있는 데이터라면 이에 해당하지 않지만요.
2. 캐시 저장 대상
3. 캐시 내 데이터 만료 기간 설정 전략
4. 캐시와 원 저장소 간 데이터 일관성 유지 전략
5. 캐시 계층의 Failover 대책 -> 분산 캐시(ex. Redis Cluster ...)
6. 캐시 메모리 크기 : 추정 필요치와 근사하게 캐시 크기를 설정할 경우, 필요한 데이터가 캐시로부터 밀려나, Cache Miss Rate 가 상승할 수 있음. -> Overprovision 으로 해결 가능(추정치보다 과할당하는 것.)
7. 캐시 데이터 방출 전략 설정

### 웹 계층 확장 : Stateful 의 한계와 Stateless의 필요성
### Geographic Locality를 고려한 저장소 확장 : 데이터-센터 도입(Feat. P2P)
### Application 확장 : 분산 시스템과 메세지 큐

## III. 개략적 규모 추정을 위해 알아야 하는 숫자들


