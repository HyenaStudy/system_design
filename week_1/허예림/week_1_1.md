페이스북이 매일 엄청난 트래픽을 버티려 memcached를 활용하여 성능을 끌어올린 내용

**memcached**

- 단순한 인메모리 캐시 프로그램
- 페이스북은 memcached를 기반으로 분산 키-값 저장소를 생성
- 해당 캐시로 초당 수십억 요청을 처리하고 수조개의 아이템을 저장하고 있다.

**Introduction**

- SNS는 트래픽이 너무 많다.
- 실시간 성이 필수이고, 한페이지에 여러 데이터가 필요함
- 인기 있는 콘텐츠 일수록 많은 사용자들이 동시에 봄
- DB연산이 너무 많이 이루어져 db 부담을 줄이기 위해 memcached를 사용
- 메모리에 저장하니까 조회 속도가 매우 빠르고, key-value 구조라 여러 종류의 데이터를 저장할 수 있다.

**Query Cache (look-aside cache)**

- 읽기 요청이 들어옴 → memcached 에 해당 키가 있는 지 조회
- 없으면 DB에 SELECT로 가져옴 → 가져온 결과를 저장

→ 그외에도 머신러닝 연산 결과를 미리 계산하여 캐시에 저장하고 여러 서비스가 그 값을 가져다 쓰게 함

memcached **구성**

- memcached 서버는 단순하게 유지
- 라우팅, 에러핸들링, 집계등은 클라이언트 서버에서 담당
- frontend cluster ↔ memcache cluster ↔ storage cluster

**Facebook의 대규모 시스템**

1. region 분산
    - 트래픽이 너무 많기 떄문에 클러스터를 여러개로 분산하고 하나의 region으로 구성
    - 특정 클러스터가 죽어도 같은 region의 다른 클러스터로 요청을 넘길 수 있도록 처리

### **클러스터 내부의 문제들**

**캐시 조회속도(latency) 향상법**

- 캐시속도 지연이 중요한 문제인 이유
- 페이스북 웹 서버가 하나의 페이지를 만들때 평균적으로 memcache에서 521개의 아이템을 가져와야함 0.5ms만 드려져도 260ms 증가 → 따라서 아주아주 빨라야함
- 위에서 얘기 했듯이 페이스북에는 수백, 수천대의 멤캐시 서버가 존재
- all-to-all communication
- 인기 데이터가 특정 서버에 몰리면 그 서버만 터지게 된다.
- 따라서 요청을 병렬로 많이 보낼 수 있도록 dag 기반 병렬화 수행
- get은 udp를 통해 더 빠르게 set/delete은 tcp를 사용
- 슬라이딩 윈도우로 트래픽 폭주 방지
- 캐시 미스가 발생하면 결국 db 조회를 수행해야함 따라서 lease 도입

**캐시가 비는 경우를 줄이기**

- lease 매커니즘을 도입하여 캐시 비는 경우를 줄임
- 문제 1 stale set
- 문제 2 Thunderign herd (떼몰림 문제)
- lease의 핵심 캐시 미스가 발생한 클라이언트 들에게 키를 set할 수 있는 허가증을 발행
- memcached는 **10초에 한 번**만 토큰을 발급하도록 설정할 수 있음.
- 그외 클라이언트들은 잠시 대기 신호를 받음, 실제 set은 몇밀리초 안에 수행되기 때문에 조금 후에 다시 요청하면 캐시에 데이터가 이미 존재함 → 수만개의 웹서버가 동시에 db로 가는 사태를 막고 단 1개의 클라이언트만 db에 접근해 키스를 채우게됨
- Thundering Herd가 많이 발생하는 key들이
    
    **DB 초당 쿼리 17K → 1.3K**로 감소
    
- 거의 **13배 감소**
- 추가로 페이스북은 일관성을 완벽하게 유지하는 것보다 부하 감소와 빠른 응답을 중요하게 여겨 삭제된 데이터를 잡시 보관해두는 recently deletedlist를 둠
- 웹 서버가 원한다면 조금 오래된 값이라도 바로 받아볼 수 있게 함
- 값이 monotonic(계속 증가하는 숫자) 형태라면 이게 크게 문제되지 않음
    - 좋아요 수, 댓글 수 등
